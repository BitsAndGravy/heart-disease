{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3612a8d",
   "metadata": {},
   "source": [
    "# Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc370d4",
   "metadata": {},
   "source": [
    "## I. Project Tasks\n",
    "\n",
    "### Define your scope\n",
    "My primary goal is to continue using the skills I learned on Codecademy. Specifically, I want to clean/tidy data, look for trends, and create a visualization that tells a story. This is a low-key project; machine learning and data mining algorithms are outside of my scope. After thinking it over, I think I will look for correlations and run statistical tests (i.e. Pearson) in Jupyter, and display the highest correlating variables in Tableau.\n",
    "\n",
    "### Decide on a question or topic\n",
    "I decided to choose a topic related to medicine - heart disease. I dispense all kinds of medications that treat heart conditions on a daily basis.\n",
    "\n",
    "### Find a dataset\n",
    "I went to kaggle and found the dataset \"Heart Disease Dataset\" provided by Mexwell (link here https://www.kaggle.com/datasets/mexwell/heart-disease-dataset).\n",
    "\n",
    "### Define your problem\n",
    "After a brief glance at the dataset and the documentation, I would like to find what makes people with heart disease different from those without heart disease. \n",
    "\n",
    "### Load and check data\n",
    "Documentation is mostly clear (the Units column consists of units and/or ranges and/or descriptions\n",
    ") and I can identify what each variable is measuring, except for the ST segment variables. Specifically, for the 'oldpeak' variable, I do not know what the numbers represents (documentation says the units are 'depression'), and how it relates to the 'ST slope' variable. I was not familiar with the term 'old peak', and decided to familiarize myself with it before getting too far along.\n",
    "\n",
    "I found that the units for _oldpeak_ are likely mm on an EKG. The first hits on Google were unhelpful, and I am making this assumption based on what I read about ST slopes.\n",
    "\n",
    "Image describing ST slopes: https://litfl.com/wp-content/uploads/2018/10/ST-segment-depression-upsloping-downsloping-horizontal.png\n",
    "\n",
    "\n",
    "### Data wrangling and tidying (in progress)\n",
    "Kaggle gave this dataset a score of 10.00 out of 10.00 for usability, but one person on Kaggle commented that about 150 people had a cholesterol level of zero, so I will review the dataset for missing values or other possible errors. \n",
    "\n",
    "### Find the story (to do)\n",
    "What, in one sentence, do you want your audience to take away from your project? You may have to create all of your visualizations, do all of your explorations, and even get feedback before you know what it is. But, take a moment to decide the one thing people should take away from your project and make that the first thing. Make that story the easiest one to find.\n",
    "\n",
    "### Communicate your findings (to do)\n",
    "This could be in the shape of a report, a Tableau Dashboard, a slide deck, etc. The best way to decide is to imagine your audience. Who are they? Where do you envision reaching them? What is the best medium for that?\\\n",
    "Do you want to reach other people interested in your topic? Do you want to persuade someone to care about something? Do you want to focus on business stakeholders? Where might you find each group? Tailor your project to that imagined scenario.\\\n",
    "From there, you can start to create a project to communicate your story with them.\n",
    "\n",
    "### Wrap up (to do)\n",
    "This step might be the most impactful for finding a job. Now that youâ€™ve written your report, created your deck, or built your dashboard, check your work. Go through and proofread. Try to view your project from wherever you decided to host it. Make sure that all of the links work, and ask a friend (or visit the forums) to find someone to go through it and tell you if there are any inconsistencies, jumps of logic, or confusing parts. This final round of checks is essential for creating a polished project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04563723",
   "metadata": {},
   "source": [
    "## II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2591e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heart_df = pd.read_csv(r'C:\\Users\\jsbit\\OneDrive\\Documents\\Coding 2023\\Git\\heart-disease\\heart_statlog_cleveland_hungary_final.csv', encoding_errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177fbff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1190 participants in this dataset.\n"
     ]
    }
   ],
   "source": [
    "print('There were', str(len(heart_df)), 'participants in this dataset.')\n",
    "# Although 1,190 participants is a lot, 'heart disease' is a large umbrella term and can affect almost anyone.\n",
    "# Therefore, I estimate that this dataset not meet power and we can't extrapolate findings to the general public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41afdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column names are:\n",
      " Index(['age', 'sex', 'chest pain type', 'resting bp s', 'cholesterol',\n",
      "       'fasting blood sugar', 'resting ecg', 'max heart rate',\n",
      "       'exercise angina', 'oldpeak', 'ST slope', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"The column names are:\\n\", heart_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ec3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of changes to variable names:\n",
    "    # Because of my German heritage, I prefer EKG instead of ECG.\n",
    "    # I see an inconsistenct in abbreviating 'bp', but spelling out 'blood sugar', so I will abbreviate them all.\n",
    "    # I think 'ST depression' is a better variable name than 'oldpeak'. \n",
    "        # I can't find much information on why it is called old peak\n",
    "        # It's a measure of the ST depression in mm on an EKG\n",
    "        # If it's a measure of depression, why is it called peak?\n",
    "\n",
    "heart_df = heart_df.rename(columns={'resting ecg': 'ekg', \n",
    "                                    'chest pain type': 'pain type', \n",
    "                                    'fasting blood sugar': 'fbs', \n",
    "                                    'max heart rate': 'max hr', \n",
    "                                    'resting bp s': 'bp',\n",
    "                                    'oldpeak': 'ST depression'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f241f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new column names are:\n",
      " Index(['age', 'sex', 'pain_type', 'bp', 'cholesterol', 'fbs', 'ekg', 'max_hr',\n",
      "       'exercise_angina', 'ST_depression', 'ST_slope', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# While I'm at it, might as well replace spaces with underscores.\n",
    "heart_df.columns = [variable.replace(' ', '_') for variable in heart_df]\n",
    "\n",
    "print(\"The new column names are:\\n\", heart_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976e041",
   "metadata": {},
   "source": [
    "## III. Exploratory Data Analysis\n",
    "### Dataset overview and descriptive statistics\n",
    "#### Descriptive statistics and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c431213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               age          sex    pain_type           bp  cholesterol  \\\n",
      "count  1190.000000  1190.000000  1190.000000  1190.000000  1190.000000   \n",
      "mean     53.720168     0.763866     3.232773   132.153782   210.363866   \n",
      "std       9.358203     0.424884     0.935480    18.368823   101.420489   \n",
      "min      28.000000     0.000000     1.000000     0.000000     0.000000   \n",
      "25%      47.000000     1.000000     3.000000   120.000000   188.000000   \n",
      "50%      54.000000     1.000000     4.000000   130.000000   229.000000   \n",
      "75%      60.000000     1.000000     4.000000   140.000000   269.750000   \n",
      "max      77.000000     1.000000     4.000000   200.000000   603.000000   \n",
      "\n",
      "               fbs          ekg       max_hr  exercise_angina  ST_depression  \\\n",
      "count  1190.000000  1190.000000  1190.000000      1190.000000    1190.000000   \n",
      "mean      0.213445     0.698319   139.732773         0.387395       0.922773   \n",
      "std       0.409912     0.870359    25.517636         0.487360       1.086337   \n",
      "min       0.000000     0.000000    60.000000         0.000000      -2.600000   \n",
      "25%       0.000000     0.000000   121.000000         0.000000       0.000000   \n",
      "50%       0.000000     0.000000   140.500000         0.000000       0.600000   \n",
      "75%       0.000000     2.000000   160.000000         1.000000       1.600000   \n",
      "max       1.000000     2.000000   202.000000         1.000000       6.200000   \n",
      "\n",
      "          ST_slope       target  \n",
      "count  1190.000000  1190.000000  \n",
      "mean      1.624370     0.528571  \n",
      "std       0.610459     0.499393  \n",
      "min       0.000000     0.000000  \n",
      "25%       1.000000     0.000000  \n",
      "50%       2.000000     1.000000  \n",
      "75%       2.000000     1.000000  \n",
      "max       3.000000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(heart_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6136052",
   "metadata": {},
   "source": [
    "* Each variable has a count of 1,190, so it's free of NAN.\n",
    "* I know that some values are zero; this is just initial exploration and I will review these numbers again later.\n",
    "* Age:\n",
    "    * I am somewhat surprised that the max age was only 77; it would be nice to know the inclusion criteria for these studies (i.e. if there was a max age for some but not others).\n",
    "    * It looks like this will have a normal distribution; the distance of the 25th and 75th percentile from the median and the distance of the min and max from the median are similar.\n",
    "* Sex: The mean and median indicate that most participants were male.\n",
    "* Resting blood pressure (systolic):\n",
    "    * Minimum was zero; not a realistic number.\n",
    "    * The average seems lower than I might have guessed. I am assuming that lots of people have high blood pressure (i.e. systolic over 130 mmHg), hopefully it isn't skewed a lot by missing/zero values.\n",
    "* Cholesterol:\n",
    "    * Minimum was zero; not a realistic number.\n",
    "    * It seems like most people had an elevated cholesterol (over 200 mg/dL)\n",
    "* Fasting blood sugar over 120 mg/dL: Looks like most people were normal (value of zero).\n",
    "* Max heart rate (during stress test and in beats per minute, I assume):\n",
    "    * I don't normally see stress test results, so this is interesting data to me.\n",
    "    * A min of 60 bpm - someone must be on some strong beta blockers or experienced angina rather quickly.\n",
    "* Exercise-induced angina: Looks like most people did not experience angina.\n",
    "* Oldpeak: This could get trippy - a negative ST depression is ST elevation (min of -2.6 mm)\n",
    "* Target: A slight majority were classified as having heart disease.    \n",
    "\n",
    "#### Data types and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f9da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1190 entries, 0 to 1189\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              1190 non-null   int64  \n",
      " 1   sex              1190 non-null   int64  \n",
      " 2   pain_type        1190 non-null   int64  \n",
      " 3   bp               1190 non-null   int64  \n",
      " 4   cholesterol      1190 non-null   int64  \n",
      " 5   fbs              1190 non-null   int64  \n",
      " 6   ekg              1190 non-null   int64  \n",
      " 7   max_hr           1190 non-null   int64  \n",
      " 8   exercise_angina  1190 non-null   int64  \n",
      " 9   ST_depression    1190 non-null   float64\n",
      " 10  ST_slope         1190 non-null   int64  \n",
      " 11  target           1190 non-null   int64  \n",
      "dtypes: float64(1), int64(11)\n",
      "memory usage: 111.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(heart_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb5920",
   "metadata": {},
   "source": [
    "* Appropriate data type:\n",
    "    * Age\n",
    "    * Blood pressure (bp)\n",
    "    * Cholesterol\n",
    "    * Fasting blood sugar (fbs)\n",
    "    * Max heart rate\n",
    "    * ST depression\n",
    "    \n",
    "* Needs changed:\n",
    "    * Sex: Binary nominal categotical\n",
    "    * Pain type: Nominal categorical\n",
    "    * EKG: Nominal categorical\n",
    "    * Exercise-induced angina: Binary nominal categorical\n",
    "    * ST slope: Ordinal categorical\n",
    "    * Target: Nominal categorical  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c0adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  pain_type   bp  cholesterol  fbs  ekg  max_hr  exercise_angina  \\\n",
      "0   40    1          2  140          289    0    0     172                0   \n",
      "1   49    0          3  160          180    0    0     156                0   \n",
      "2   37    1          2  130          283    0    1      98                0   \n",
      "3   48    0          4  138          214    0    0     108                1   \n",
      "4   54    1          3  150          195    0    0     122                0   \n",
      "\n",
      "   ST_depression  ST_slope  target  \n",
      "0            0.0         1       0  \n",
      "1            1.0         2       1  \n",
      "2            0.0         1       0  \n",
      "3            1.5         2       1  \n",
      "4            0.0         1       0  \n"
     ]
    }
   ],
   "source": [
    "print(heart_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d483969",
   "metadata": {},
   "source": [
    "#### Checking for duplicates and handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddf1fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index  age  sex  pain_type   bp  cholesterol  fbs  ekg  max_hr  \\\n",
      "0      163   49    0          2  110          208    0    0     160   \n",
      "1      604   58    1          3  150          219    0    1     118   \n",
      "2      887   63    1          1  145          233    1    2     150   \n",
      "3      888   67    1          4  160          286    0    2     108   \n",
      "4      889   67    1          4  120          229    0    2     129   \n",
      "..     ...  ...  ...        ...  ...          ...  ...  ...     ...   \n",
      "267   1156   42    1          3  130          180    0    0     150   \n",
      "268   1157   61    1          4  140          207    0    2     138   \n",
      "269   1158   66    1          4  160          228    0    2     138   \n",
      "270   1159   46    1          4  140          311    0    0     120   \n",
      "271   1160   71    0          4  112          149    0    0     125   \n",
      "\n",
      "     exercise_angina  ST_depression  ST_slope  target  \n",
      "0                  0            0.0         1       0  \n",
      "1                  1            0.0         2       1  \n",
      "2                  0            2.3         3       0  \n",
      "3                  1            1.5         2       1  \n",
      "4                  1            2.6         2       1  \n",
      "..               ...            ...       ...     ...  \n",
      "267                0            0.0         1       0  \n",
      "268                1            1.9         1       1  \n",
      "269                0            2.3         1       0  \n",
      "270                1            1.8         2       1  \n",
      "271                0            1.6         2       0  \n",
      "\n",
      "[272 rows x 13 columns]\n",
      "0       163\n",
      "1       604\n",
      "2       887\n",
      "3       888\n",
      "4       889\n",
      "5       890\n",
      "6       891\n",
      "7       892\n",
      "8       893\n",
      "9       894\n",
      "10      895\n",
      "11      896\n",
      "12      897\n",
      "13      898\n",
      "14      899\n",
      "15      900\n",
      "16      901\n",
      "17      902\n",
      "18      903\n",
      "19      904\n",
      "20      905\n",
      "21      906\n",
      "22      907\n",
      "23      908\n",
      "24      909\n",
      "25      910\n",
      "26      911\n",
      "27      912\n",
      "28      913\n",
      "29      914\n",
      "30      915\n",
      "31      916\n",
      "32      917\n",
      "33      918\n",
      "34      919\n",
      "35      920\n",
      "36      921\n",
      "37      922\n",
      "38      923\n",
      "39      924\n",
      "40      925\n",
      "41      926\n",
      "42      927\n",
      "43      928\n",
      "44      929\n",
      "45      930\n",
      "46      931\n",
      "47      932\n",
      "48      933\n",
      "49      934\n",
      "50      935\n",
      "51      936\n",
      "52      937\n",
      "53      938\n",
      "54      939\n",
      "55      940\n",
      "56      941\n",
      "57      942\n",
      "58      943\n",
      "59      944\n",
      "60      945\n",
      "61      946\n",
      "62      947\n",
      "63      948\n",
      "64      949\n",
      "65      950\n",
      "66      951\n",
      "67      952\n",
      "68      953\n",
      "69      954\n",
      "70      955\n",
      "71      956\n",
      "72      957\n",
      "73      958\n",
      "74      959\n",
      "75      960\n",
      "76      961\n",
      "77      962\n",
      "78      963\n",
      "79      964\n",
      "80      965\n",
      "81      966\n",
      "82      967\n",
      "83      968\n",
      "84      969\n",
      "85      970\n",
      "86      971\n",
      "87      972\n",
      "88      973\n",
      "89      975\n",
      "90      976\n",
      "91      977\n",
      "92      978\n",
      "93      979\n",
      "94      980\n",
      "95      981\n",
      "96      982\n",
      "97      983\n",
      "98      984\n",
      "99      985\n",
      "100     986\n",
      "101     987\n",
      "102     988\n",
      "103     989\n",
      "104     990\n",
      "105     991\n",
      "106     992\n",
      "107     993\n",
      "108     994\n",
      "109     995\n",
      "110     996\n",
      "111     997\n",
      "112     998\n",
      "113     999\n",
      "114    1000\n",
      "115    1001\n",
      "116    1002\n",
      "117    1003\n",
      "118    1004\n",
      "119    1005\n",
      "120    1006\n",
      "121    1007\n",
      "122    1008\n",
      "123    1009\n",
      "124    1010\n",
      "125    1011\n",
      "126    1012\n",
      "127    1013\n",
      "128    1014\n",
      "129    1015\n",
      "130    1016\n",
      "131    1017\n",
      "132    1018\n",
      "133    1019\n",
      "134    1020\n",
      "135    1021\n",
      "136    1022\n",
      "137    1023\n",
      "138    1024\n",
      "139    1025\n",
      "140    1026\n",
      "141    1027\n",
      "142    1028\n",
      "143    1029\n",
      "144    1030\n",
      "145    1031\n",
      "146    1032\n",
      "147    1033\n",
      "148    1034\n",
      "149    1035\n",
      "150    1036\n",
      "151    1037\n",
      "152    1038\n",
      "153    1039\n",
      "154    1040\n",
      "155    1041\n",
      "156    1042\n",
      "157    1043\n",
      "158    1044\n",
      "159    1045\n",
      "160    1046\n",
      "161    1047\n",
      "162    1048\n",
      "163    1049\n",
      "164    1050\n",
      "165    1051\n",
      "166    1052\n",
      "167    1054\n",
      "168    1055\n",
      "169    1056\n",
      "170    1057\n",
      "171    1058\n",
      "172    1059\n",
      "173    1060\n",
      "174    1061\n",
      "175    1062\n",
      "176    1063\n",
      "177    1064\n",
      "178    1065\n",
      "179    1066\n",
      "180    1067\n",
      "181    1068\n",
      "182    1069\n",
      "183    1070\n",
      "184    1071\n",
      "185    1072\n",
      "186    1073\n",
      "187    1074\n",
      "188    1075\n",
      "189    1076\n",
      "190    1077\n",
      "191    1078\n",
      "192    1080\n",
      "193    1081\n",
      "194    1082\n",
      "195    1083\n",
      "196    1084\n",
      "197    1085\n",
      "198    1086\n",
      "199    1087\n",
      "200    1088\n",
      "201    1089\n",
      "202    1090\n",
      "203    1091\n",
      "204    1092\n",
      "205    1093\n",
      "206    1094\n",
      "207    1095\n",
      "208    1096\n",
      "209    1097\n",
      "210    1098\n",
      "211    1099\n",
      "212    1100\n",
      "213    1101\n",
      "214    1102\n",
      "215    1103\n",
      "216    1104\n",
      "217    1105\n",
      "218    1106\n",
      "219    1107\n",
      "220    1108\n",
      "221    1109\n",
      "222    1110\n",
      "223    1111\n",
      "224    1112\n",
      "225    1113\n",
      "226    1114\n",
      "227    1115\n",
      "228    1116\n",
      "229    1117\n",
      "230    1118\n",
      "231    1119\n",
      "232    1120\n",
      "233    1121\n",
      "234    1122\n",
      "235    1123\n",
      "236    1124\n",
      "237    1125\n",
      "238    1126\n",
      "239    1127\n",
      "240    1128\n",
      "241    1129\n",
      "242    1130\n",
      "243    1131\n",
      "244    1132\n",
      "245    1133\n",
      "246    1134\n",
      "247    1135\n",
      "248    1136\n",
      "249    1137\n",
      "250    1138\n",
      "251    1139\n",
      "252    1140\n",
      "253    1141\n",
      "254    1142\n",
      "255    1143\n",
      "256    1144\n",
      "257    1145\n",
      "258    1146\n",
      "259    1147\n",
      "260    1148\n",
      "261    1149\n",
      "262    1150\n",
      "263    1151\n",
      "264    1152\n",
      "265    1154\n",
      "266    1155\n",
      "267    1156\n",
      "268    1157\n",
      "269    1158\n",
      "270    1159\n",
      "271    1160\n"
     ]
    }
   ],
   "source": [
    "duplicated_hearts = heart_df[heart_df.duplicated()].reset_index()\n",
    "print(duplicated_hearts)\n",
    "# The index numbers seem interesting in how they are consecutive - are they all that way?\n",
    "print(duplicated_hearts['index'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b66379",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       163\n",
      "1       604\n",
      "2       887\n",
      "3       888\n",
      "4       889\n",
      "       ... \n",
      "267    1156\n",
      "268    1157\n",
      "269    1158\n",
      "270    1159\n",
      "271    1160\n",
      "Name: index, Length: 272, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(duplicated_hearts['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94939d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "# Yes, it looks like each row after row 887 in heart_df is a duplicate. Calculating if visual analysis was accurate:\n",
    "dup_range = 1160-887 + 1\n",
    "other_dups = 2\n",
    "dup_count_check = dup_range + other_dups\n",
    "print(dup_count_check)\n",
    "# Since my calculated guess of 276 is higher than 272, it looks like not all rows in the range 887-1161 are duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994a4f2",
   "metadata": {},
   "source": [
    "##### What percent of the data are duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bff17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 22.86 % of the data is duplicated.\n"
     ]
    }
   ],
   "source": [
    "perc_dup = round(len(duplicated_hearts) / len(heart_df) * 100, 2)\n",
    "print('About', str(perc_dup), '% of the data is duplicated.')\n",
    "# Almost a quarter of the data is duplicated - that is a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20cb449",
   "metadata": {},
   "source": [
    "##### How many rows would be left after dropping duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab4b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 918 rows remaining after dropping duplicates.\n"
     ]
    }
   ],
   "source": [
    "remainder = len(heart_df) - len(duplicated_hearts)\n",
    "print('There will be', str(remainder), 'rows remaining after dropping duplicates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb54cd",
   "metadata": {},
   "source": [
    "##### Discussion on handling duplicates: \n",
    "The likelihood of having so many duplicates, and the fact that they are basically all grouped together, I think it is safe to drop the duplicates.\\\n",
    "It would have been nice to have a 'trial ID' or 'participant ID' variable, since this dataset is a conglomeration of multiple datasets. I think this would help pinpoint why there are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a81f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918\n"
     ]
    }
   ],
   "source": [
    "heart_nd_df = heart_df.drop_duplicates()\n",
    "print(len(heart_nd_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4d93c",
   "metadata": {},
   "source": [
    "Next step: looking at inappropriate zeros (ex. cholesterol, bp, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
